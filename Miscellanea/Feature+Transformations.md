**For categorical features**

1. Encoding (lexical)

   For each column, after combining train and test set, sort the unique values by the length and value itself. And then replace that column content by sequence index.

   ​

2. Dummy

   For each column, calculate the frequency of each content occur in the train set; Set a minimum frequency threshold, and for each categorical content in that column, if the frequency is larger than the threshold, add several columns (number of the unique categorical content) using one-hot encoding in both train and test set; and if the frequency is smaller than the threshold, add only one column that encode all the rare categorical contents in both train and test set.

   ​

3. Counts

   For each column, after combining train and test set, calculate the frequency of each categorical content, and replace that column content by the corresponding frequency both in train and test set.

   ​



**For numeric features**

1. Boxcox

   Combine train and test set, and calculate the skewness of each column (using skew in scipy.stats); if the skewness is larger than some specific threshold (like 0.25), apply boxcox transform on that column (always add 1 to the values to make positive as adding a constant value don't change the boxcox result).

   ​

2. Skew

   Combine train and test set, and find the unskew continuous columns; try different transformations on each minmax_scaled column to make it skew.

   ​

3. Rank and Normalization

   Combine train and test set, and apply rank transformation on each column (using rankdata in scipy.stats.mstats); scale that ranked column into [-1, 1] (using minmax_scale in sklearn.preprocessing); make it gussian (using erfinv in scipy.special) and also scale it (using scale in sklearn.preprocessing).

   ​

4. Scale

   Combine train and test set, and apply scale in sklearn.preprocessing.

   ​

5. Remove correlation

   Try to find pairwise two columns with high correlation after combining train and test set, and apply subtracting to remove the correlation.

   ​



**Feature Space**

1. MiniBatchKMeans

   Bind categorical dummy and raw numeric, as well as bind train and test set; then fit MiniBatchKMeans (k = 25, 50, 75, 100, 200) on the whole data and apply kmeans transform (using kmeans.transform in sklearn.cluster) -> multiply negative gamma -> apply np.exp. And when fitting single model for predicting, one can combine all the feature space generated by different k.

   ​

2. TruncatedSVD

   Bind categorical dummy and raw numeric, as well as bind train and test set; apply TruncatedSVD transform on the whole data set to change it into n components (find the number n that can possibly maximum the explain of variance) feature space. (Using TruncatedSVD in sklearn.decomposition) 

   ​



**For y transformation in regression**

1. np.log(y + ofs)
2. boxcox(np.log1p(y), lmbda=norm_y_lambda)    (norm_ylambda = 0.7)
3. (y + ofs) ** p





**Combination with Better performance** 

1. For linear regression with Ridge

   *  Scaled and dummy features with y transform
   *  Kmeans and Truncated-svd transformation feature space with y transform

   ​

2. For ExtraTrees, Random Forest, Gradient Boosting and AdaBoost regression

   *  Only used encoded categorical and raw numeric features with y transform
   *  Based on the above method, will get a much better performance when adding pair-wised combinations of categorical features and also apply encoded transformation

   ​

3. For KNN regressor

   *  Only used encoded categorical and raw numeric features with y transform

      ```python
      Pipeline([('sc', StandardScaler()), ('est', KNeighborsRegressor(20, n_jobs=-1))])
      ```



4. For SVR

   *  Only used encoded categorical and raw numeric features with y transform

      ```python
      Pipeline([('sc', StandardScaler()), ('est', SVR())])
      ```



5. For xgboost 

   -  Only used encoded categorical and raw numeric features with y transform
   -  Based on the above method, will get a much better performance when adding pair-wised combinations of categorical features and also apply encoded transformation

   ​

6. For Keras (NN)

   *    ​
   *    ​




To find the categorical interaction features, one can extract from xgboost models by just trying the most important categorical features, or better, use the xgbfi tool.



To improve model performance:

1.  Averaging multiple runs of different seeds (help to reduce model variance)
2. Adding categorical combinations
3. For neural networks, applying exponential moving average to weights of single network [this implementation](https://gist.github.com/soheilb/c5bf0ba7197caa095acfcb69744df756); adding batch normalization and dropout
4. Adding SVD and cluster features



To tune model parameters:  [Kaggle forum](https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/20662/) 

-  Manual tuning, which works good when you have some intuition about parameter behaviour and may estimate model performance before its training completes by per-epoch validation scores;
-  Uninformed parameter search - using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) or [RandomizedSearch](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.RandomizedSearchCV.html) from sklearn package, or similar - most simple of all;
-  Informed search using [HyperOpt](http://hyperopt.github.io/hyperopt/) or [BayesOptimization](https://github.com/fmfn/BayesianOptimization/) or similar package - it tries to fit some model to scores of different parameter sets and selects the most promising point for each next try - so it usually finds the optimum a lot faster than uninformed search.



**Generally, the rank of the average performance of all the algorithm above is: xgboost > nn >  gradient boosting > adaboost > randomforest > extraTree > SVM > lr (but lr with feature space sometimes has a better performance) > knn**



