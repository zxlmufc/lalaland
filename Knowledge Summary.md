##### Loss Function and Cost Function

Measure the penalty of inaccuracy

1. Square loss

   $L(Y, f(X)) = (Y-f(x))^2$ always used in linear regression, where the $(Y-f(x))^2$ is the residual sum of squares (RSS). 

   While cost function is a more general object, the cost function for square loss is **Mean Square Loss**: $MSE(\theta) = \frac{1}{N}\sum_{i=1}^{n}(Y-f(x))^2$

   ​

   ​

2. Hingle loss

3. Logistic loss

4. Cross entropy loss

5. 0/1 loss



##### Regularization

##### Gradient Descent

##### Metric Measure

##### Unbalanced Sample

##### Discrete and Continuous Distribution

##### Information Theory

##### Boosting

##### Feature Reduction

##### Ensemble Method

##### Monte Carlo



##### Deep Learning

1. Back propagation
2. AutoEncoder

##### Neural Network

##### Tree-based Model

##### Support Vector Machine

##### Linear and Logistic

##### Naive Bayes

